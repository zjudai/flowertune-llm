[build-system]
requires = [ "hatchling",]
build-backend = "hatchling.build"

[project]
name = "flwr-nlp"
version = "1.0.0"
description = ""
license = "Apache-2.0"
dependencies = [ "flwr[simulation]>=1.16.0", "flwr-datasets>=0.5.0", "torch==2.3.1", "trl==0.8.1", "bitsandbytes==0.45.0", "scipy==1.13.0", "peft==0.6.2", "transformers==4.47.0", "sentencepiece==0.2.0", "omegaconf==2.3.0", "hf_transfer==0.1.8",]

[tool.flwr.app]
publisher = "taoshen"

[tool.flwr.federations]
default = "local-simulation"

[tool.flwr.app.components]
serverapp = "flwr_nlp.server_app:app"
clientapp = "flwr_nlp.client_app:app"

[tool.flwr.app.config]
num-server-rounds = 10

[tool.flwr.app.config.model]
name = "Qwen/Qwen2.5-1.5B-Instruct"
quantization = 4
gradient-checkpointing = true

[tool.flwr.app.config.train]
save-every-round = 5
learning-rate-max = 5e-5
learning-rate-min = 1e-6
seq-length = 512

[tool.flwr.app.config.strategy]
fraction-fit = 0.1
fraction-evaluate = 0.0

[tool.flwr.federations.local-simulation.options]
num-supernodes = 50

[tool.hatch.build.targets.wheel]
packages = [ ".",]

[tool.flwr.app.config.model.lora]
peft-lora-r = 32
peft-lora-alpha = 64

[tool.flwr.app.config.train.training-arguments]
output-dir = ""
learning-rate = ""
per-device-train-batch-size = 8
gradient-accumulation-steps = 1
logging-steps = 10
num-train-epochs = 3
max-steps = 10
save-steps = 1000
save-total-limit = 10
gradient-checkpointing = true
lr-scheduler-type = "constant"

[tool.flwr.app.config.static.dataset]
name = "FinGPT/fingpt-sentiment-train"

[tool.flwr.federations.local-simulation.options.backend.client-resources]
num-cpus = 6
num-gpus = 1.0
